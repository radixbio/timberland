job "kafka-daemons" {
  namespace = "${namespace}"
  all_at_once = false

  constraint {
    attribute = "$${attr.kernel.name}"
    operator = "="
    value = "linux"
  }

  constraint {
    operator = "distinct_hosts"
    value = "true"
  }

  datacenters = ["dc1"]

%{ for i in range(dev ? 1 : quorum_size) ~}
  group "kafka-${i}" {

    count = 1

    volume "kafka_data" {
      type = "host"
      source = "kafka_data"
      read_only = false
    }

    //hack to (try to) pin reallocations to the same node
    //it can fail!
    ephemeral_disk {
      sticky = true
    }

    task "kafka" {

      volume_mount {
        volume = "kafka_data"
        destination = "/var/lib/kafka/data"
        read_only = false
      }

      config {
        image = "confluentinc/cp-kafka:5.3.1"
        auth_soft_fail = false
        privileged = false
        cap_add = []
        //TODO read meta.properties here and use that if it exists
      }

      driver = "docker"
      env = {
        "KAFKA_ADVERTISED_HOST_NAME" = "kafka-${i}.service.consul",
        // we need to increment the listeners and advertised listeners because zookeeper gets sad if multiple kafka nodes have the same advertised listeners
        "KAFKA_ADVERTISED_LISTENERS" = "INSIDE://127.0.0.1:${interbroker_port + i},OUTSIDE://127.0.0.1:${9092 + i}",
        // KAFKA_LISTENERS actually tells kafka what host:port to bind to
        "KAFKA_LISTENERS" = "INSIDE://0.0.0.0:${interbroker_port + i},OUTSIDE://0.0.0.0:${9092 + i}",
        // kafka wants to be able to talk to all the zookeepers
        "KAFKA_ZOOKEEPER_CONNECT" = "%{ for j in range(dev ? 1 : zookeeper_quorum_size) ~}$${NOMAD_UPSTREAM_ADDR_zookeeper-client-${j}},%{ endfor ~}",
        "KAFKA_LISTENER_SECURITY_PROTOCOL_MAP" = "INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT",
        "TOPIC_AUTO_CREATE" = "true",
        "KAFKA_INTER_BROKER_LISTENER_NAME" = "OUTSIDE",
        "KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR" = "${ dev ? 1 : min(quorum_size, 3) }",
        //TODO read meta.properties here and use that if it exists
      }
      kill_timeout = "5s"
      kill_signal = "SIGINT"
      leader = false

      resources {
        cpu = 1000
        memory = 2048
      }

      shutdown_delay = "0s"
    }

    network {
      mbits = 10
      mode = "bridge"
    }

    service {
      name = "kafka-${i}"
      port = ${ 9092 + i }
      tags = ["kafka-quorum","kafka-plaintext", "system"]
      address_mode = "auto"

      connect {
        sidecar_service {
          proxy {

            %{ for j in range(dev ? 1 : zookeeper_quorum_size) ~}
            upstreams {
              destination_name = "zookeeper-client-${j}"
              local_bind_port = ${ 2181 + j }
            }
            %{ endfor ~}

            %{ for j in range(dev ? 1 : quorum_size) ~}
            ${ i == j ? "" : <<-EOT
            upstreams {
              destination_name = "kafka-${j}"
              local_bind_port = ${ 9092 + j}
            }
            EOT
            }
            %{ endfor ~}
          }
        }
      }

      check {
        type = "script"
        name = "kafka-health-${i}"
        task = "kafka"
        command = "bash"
        // the broker id assigned by zookeeper gets written to meta.properties. if that file doesn't exist, something has gone wrong
        // otherwise, check that this kafka's broker id is contained in the list of kafka nodes given by zookeeper
        args = ["-c", "if [ -f /var/lib/kafka/data/meta.properties ]; then echo dump | nc localhost 2181 | grep /brokers/ids/$(cat /var/lib/kafka/data/meta.properties  | grep broker.id | sed 's/^[^=]*=//g'); else echo 'meta.properties file not found'; exit 1; fi"]
        interval = "10s"
        timeout = "5s"
      }
    }
  }

%{ endfor ~}

  priority = 50
  region = "global"
  type = "service"

  update {
    max_parallel = 1
    health_check = "checks"
    min_healthy_time = "10s"
    healthy_deadline = "5m"
    progress_deadline = "10m"
    auto_revert = false
    canary = 0
    stagger = "10s"
  }

}
